/home/mh2803/miniconda3/envs/simlingo/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[rank: 0] Seed set to 42
Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. 
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:12<00:38, 12.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:22<00:22, 11.06s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:32<00:10, 10.56s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:33<00:00,  6.62s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:33<00:00,  8.28s/it]
/home/mh2803/miniconda3/envs/simlingo/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:208: Attribute 'vision_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['vision_model'])`.
/home/mh2803/miniconda3/envs/simlingo/lib/python3.8/site-packages/pytorch_lightning/utilities/parsing.py:208: Attribute 'language_model' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['language_model'])`.
/home/mh2803/miniconda3/envs/simlingo/lib/python3.8/site-packages/lightning_fabric/plugins/environments/slurm.py:204: The `srun` command is available on your system but is not used. HINT: If your intention is to run Lightning on SLURM, prepend your python command with `srun` like so: srun python train.py experiment=cluster_training gpus=2 strategy ...
Using 16bit Automatic Mixed Precision (AMP)
Trainer already configured with model summary callbacks: [<class 'pytorch_lightning.callbacks.model_summary.ModelSummary'>]. Skipping setting a default `ModelSummary` callback.
GPU available: True (cuda), used: True
TPU available: False, using: 0 TPU cores
HPU available: False, using: 0 HPUs
Initializing distributed: GLOBAL_RANK: 0, MEMBER: 1/2
/home/mh2803/miniconda3/envs/simlingo/lib/python3.8/site-packages/hydra/_internal/hydra.py:119: UserWarning: Future Hydra versions will no longer change working directory at job runtime by default.
See https://hydra.cc/docs/1.2/upgrades/1.1_to_1.2/changes_to_job_working_dir/ for more information.
  ret = run_job(
[rank: 1] Seed set to 42
Some kwargs in processor config are unused and will not have any effect: num_additional_image_tokens. 
Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]Loading checkpoint shards:  25%|â–ˆâ–ˆâ–Œ       | 1/4 [00:04<00:12,  4.19s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 2/4 [00:07<00:06,  3.50s/it]Loading checkpoint shards:  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 3/4 [00:09<00:03,  3.02s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  1.99s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:10<00:00,  2.52s/it]
Initializing distributed: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All distributed processes registered. Starting with 2 processes
----------------------------------------------------------------------------------------------------

wandb: Currently logged in as: mhphoenix (mhphoenix-rochester-institute-of-technology). Use `wandb login --relogin` to force relogin
wandb: wandb version 0.24.0 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.3
wandb: Run data is saved locally in ./wandb/run-20260125_130916-2026_01_25_13_07_46_cluster_training
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run 2026_01_25_13_07_46_cluster_training
wandb: â­ï¸ View project at https://wandb.ai/mhphoenix-rochester-institute-of-technology/simlingo_base
wandb: ðŸš€ View run at https://wandb.ai/mhphoenix-rochester-institute-of-technology/simlingo_base/runs/2026_01_25_13_07_46_cluster_training
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
Loading `train_dataloader` to estimate number of stepping batches.

   | Name                                             | Type                         | Params | Mode 
-----------------------------------------------------------------------------------------------------------
0  | vision_model                                     | LLaVAnextEncoderModel        | 326 M  | train
1  | vision_model.image_encoder                       | LingoLlavaNextModel          | 324 M  | eval 
2  | vision_model.image_encoder.vision_tower          | CLIPVisionModel              | 303 M  | eval 
3  | vision_model.image_encoder.multi_modal_projector | LlavaNextMultiModalProjector | 21.0 M | eval 
4  | vision_model.projection                          | Linear                       | 2.1 M  | train
5  | language_model                                   | Llama                        | 50.3 M | train
6  | language_model.model                             | LlamaModel                   | 50.3 M | train
7  | language_model.model.layers                      | ModuleList                   | 50.3 M | train
8  | language_model.model.norm                        | LlamaRMSNorm                 | 512    | train
9  | language_model.model.rotary_emb                  | LlamaRotaryEmbedding         | 0      | train
10 | adaptors                                         | AdaptorList                  | 279 K  | train
11 | adaptors.driving                                 | DrivingAdaptor               | 279 K  | train
12 | adaptors.driving.route_head                      | Sequential                   | 131 K  | train
13 | adaptors.driving.speed_wps_head                  | Sequential                   | 131 K  | train
14 | speed_encoder                                    | VectorInputAdaptor           | 132 K  | train
15 | speed_encoder.norm_layer                         | NormZeroOne                  | 0      | train
16 | speed_encoder.mlp                                | Sequential                   | 132 K  | train
17 | speed_encoder.mlp.0                              | Linear                       | 512    | train
18 | speed_encoder.mlp.1                              | ReLU                         | 0      | train
19 | speed_encoder.mlp.2                              | Linear                       | 131 K  | train
20 | route_encoder                                    | WaypointInputAdaptor         | 132 K  | train
21 | route_encoder.norm_layer                         | NormZeroOne                  | 0      | train
22 | route_encoder.mlp                                | Sequential                   | 132 K  | train
23 | route_encoder.mlp.0                              | Linear                       | 768    | train
24 | route_encoder.mlp.1                              | ReLU                         | 0      | train
25 | route_encoder.mlp.2                              | Linear                       | 131 K  | train
26 | language_projection                              | Identity                     | 0      | train
-----------------------------------------------------------------------------------------------------------
377 M     Trainable params
0         Non-trainable params
377 M     Total params
1,509.912 Total estimated model params size (MB)
198       Modules in train mode
302       Modules in eval mode
/home/mh2803/miniconda3/envs/simlingo/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:431: It is recommended to use `self.log('val/loss', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.
[2026-01-25T13:25:25.093] error: *** JOB 21029727 ON skl-a-54 CANCELLED AT 2026-01-25T13:25:25 DUE TO TIME LIMIT ***
