# @package _global_
# Cluster训练配置（用于大规模数据集）
# 数据路径指向: /shared/rc/llm-gen-agent/mhu/simlingo_dataset/database
defaults:
  - /data_module: carla_no_buckets  # 不使用 bucket 过滤，适合小规模数据集
  - /model/vision_model: llavanext

model:
  lr: 3e-5
  vision_lr: 3e-5
  predict_route_as_wps: True
  speed_wps_mode: '2d'
  language_model:
    variant: tiny
    lora: False
    _target_: simlingo_base_training.models.language_model.llama.Llama
  vision_model:
    downsample_feature_grid_factor: 2

data_module:
  # Cluster数据路径（绝对路径）
  data_path: /mnt/localssd/simlingo_extracted
  bucket_path: /shared/rc/llm-gen-agent/mhu/simlingo_dataset/database/bucketsv2_simlingo
  batch_size: 32  # 根据GPU显存调整（单GPU A100 40GB建议8-16）
  num_workers: 16  # 根据CPU核心数调整（cluster通常有更多CPU）
  train_partitions: null  # 不使用 bucket 过滤，使用所有数据
  route_as: target_point
  hist_len: 1
  cut_bottom_quarter: False
  use_global_img: False
  img_shift_augmentation: True
  img_shift_augmentation_prob: 0.5
  image_enhancing: True

max_epochs: 30
val_every_n_epochs: 10
seed: 42
gpus: 1  # 单GPU训练（根据SBATCH --gpus-per-node设置）
name: adobe_training
enable_wandb: true  # 启用 wandb

hydra:
  run:
    dir: outputs/${wandb_name}_${name}

